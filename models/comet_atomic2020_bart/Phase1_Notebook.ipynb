{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d00e37c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21156/925666532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded54dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = a.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    return torch.dot(a,b) / (torch.norm(a) * torch.norm(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comet:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        task = \"summarization\"\n",
    "        use_task_specific_params(self.model, task)\n",
    "        self.batch_size = 1\n",
    "        self.decoder_start_token_id = None\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            queries,\n",
    "            decode_method=\"beam\",\n",
    "            num_generate=5,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Given list of queries, this generates num_generate sentences for each query.\n",
    "        Returns:\n",
    "            decs: list of lists of generated sentences (B lists)\n",
    "            decs_embeddings [B, L, H]: embeddings for the sentences\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            examples = queries\n",
    "\n",
    "            decs = []\n",
    "            decs_embeddings = []\n",
    "            for batch in list(chunks(examples, self.batch_size)):\n",
    "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "\n",
    "                summaries = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_start_token_id=self.decoder_start_token_id,\n",
    "                    num_beams=num_generate,\n",
    "                    num_return_sequences=num_generate,\n",
    "                    )\n",
    "\n",
    "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                decs.append(dec)\n",
    "\n",
    "                # converts list of strings to dictionary containing input_ids, masks, etc for input to transformer\n",
    "                dec_batch = self.tokenizer(dec, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "                dec_input_ids, _ = trim_batch(**dec_batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "                dec_embeddings = self.ids2embeddings(dec_input_ids)\n",
    "                decs_embeddings.append(dec_embeddings)\n",
    "        \n",
    "            # Return output as string as well as string converted to embeddings\n",
    "            # [B, L, H]\n",
    "            decs_embeddings = torch.cat(decs_embeddings, dim=1)\n",
    "            return decs, decs_embeddings\n",
    "    \n",
    "    def ids2embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (pytorch tensor [B, L])\n",
    "        Returns:\n",
    "            result (pytorch tensor [B, L, H]) where H is output dimension of embedding layer\n",
    "        \"\"\"\n",
    "        return self.model.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    def str2embeddings(self, sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: list of strings, each string is a sentence\n",
    "        Returns:\n",
    "            result (pytorch tensor [B, L, H]): B is number of sentences, L is max length of sentences, H is number of dimensions of embedding layer\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer(sentences, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "        input_ids, _ = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "        embeddings = self.ids2embeddings(input_ids)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a141e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_bfs(start_sentence, end_sentence, top_k, thresh=0.95, model_path=\"comet-atomic_2020_BART\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        top_k: how many\n",
    "    \"\"\"\n",
    "    print(\"model loading ...\")\n",
    "    comet = Comet(model_path)\n",
    "    comet.model.zero_grad()\n",
    "    print(\"model loaded\")\n",
    "\n",
    "\n",
    "    embeddings = comet.str2embeddings([start_sentence, end_sentence])\n",
    "    start_embedding = embeddings[0]\n",
    "    end_embedding = embeddings[1]\n",
    "\n",
    "    forward_q = [(start_sentence, start_embedding)]\n",
    "    backward_q = [(end_sentence, end_embedding)]\n",
    "\n",
    "    all_forward_results = []\n",
    "    all_backward_results = []\n",
    "\n",
    "    while len(forward_q) > 0 or len(backward_q) > 0:\n",
    "        if len(forward_q) > 0:\n",
    "            forward_sentence, forward_embedding = forward_q.pop(0)\n",
    "            all_forward_results.append(forward_sentence)\n",
    "            rel = \"isBefore\"\n",
    "            queries = [\"{} {} [GEN]\".format(forward_sentence, rel)]\n",
    "            forward_results, _ = comet.generate(queries, decode_method=\"beam\", num_generate=top_k)\n",
    "        \n",
    "        if len(backward_q) > 0:\n",
    "            backward_sentence, backward_embedding = backward_q.pop(0)\n",
    "            all_backward_results.append(backward_sentence)\n",
    "            rel = \"isAfter\"\n",
    "            queries = [\"{} {} [GEN]\".format(backward_sentence, rel)]\n",
    "            backward_results, _ = comet.generate(queries, decode_method=\"beam\", num_generate=top_k)\n",
    "        \n",
    "        similarity_score = cosine_similarity(forward_embedding, backward_embedding)\n",
    "        if similarity_score >= thresh:\n",
    "            print(\"{} and {} had similarity score of {}\".format(forward_sentence, backward_sentence, similarity_score))\n",
    "            final_forward_sentences = backtrack(all_forward_results, top_k)\n",
    "            final_backward_sentences = backtrack(all_backward_results, top_k)\n",
    "\n",
    "            return final_forward_sentences + final_backward_sentences[::-1][1:]\n",
    "        \n",
    "        # Need to ensure forward and backward sentences have same dimensions for computing\n",
    "        # cosine similarity\n",
    "        for forward_result, backward_result in zip(forward_results[0], backward_results[0]):\n",
    "            forward_embedding, backward_embedding = comet.str2embeddings([forward_result, backward_result])\n",
    "            forward_q.append((forward_result, forward_embedding))\n",
    "            backward_q.append((backward_result, backward_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(all_sentences, b):\n",
    "    \"\"\"\n",
    "    Do integer math to backtrack/recover path taken to reach the last sentence\n",
    "    Starting from the last node reached in the graph using BFS, where index represents\n",
    "    the ith node that was discovered in the graph during the bfs search (root node is index == 0)\n",
    "    the next index would be given by this equation: index := ceil(index / b) - 1\n",
    "    where b is branching factor or number of children each node has. We are taking advantage\n",
    "    of the fact that every node has b children. In our case, b = top_k where top_k is number of sentences we chose to generate for each query.\n",
    "    \"\"\"\n",
    "\n",
    "    # index starts at last node discovered which terminated our bfs search\n",
    "    ind = len(all_sentences) - 1\n",
    "    result = []\n",
    "    # now let's backtrack to recover the path taken to reach this terminating node\n",
    "    while ind != 0:\n",
    "        result.append(all_sentences[ind]) \n",
    "        ind = math.ceil(float(ind) / float(b)) - 1\n",
    "    # don't forget to add root-node\n",
    "    result.append(all_sentences[0])\n",
    "    return result[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ed305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_relations = [\n",
    "    \"AtLocation\",\n",
    "    \"CapableOf\",\n",
    "    \"Causes\",\n",
    "    \"CausesDesire\",\n",
    "    \"CreatedBy\",\n",
    "    \"DefinedAs\",\n",
    "    \"DesireOf\",\n",
    "    \"Desires\",\n",
    "    \"HasA\",\n",
    "    \"HasFirstSubevent\",\n",
    "    \"HasLastSubevent\",\n",
    "    \"HasPainCharacter\",\n",
    "    \"HasPainIntensity\",\n",
    "    \"HasPrerequisite\",\n",
    "    \"HasProperty\",\n",
    "    \"HasSubEvent\",\n",
    "    \"HasSubevent\",\n",
    "    \"HinderedBy\",\n",
    "    \"InheritsFrom\",\n",
    "    \"InstanceOf\",\n",
    "    \"IsA\",\n",
    "    \"LocatedNear\",\n",
    "    \"LocationOfAction\",\n",
    "    \"MadeOf\",\n",
    "    \"MadeUpOf\",\n",
    "    \"MotivatedByGoal\",\n",
    "    \"NotCapableOf\",\n",
    "    \"NotDesires\",\n",
    "    \"NotHasA\",\n",
    "    \"NotHasProperty\",\n",
    "    \"NotIsA\",\n",
    "    \"NotMadeOf\",\n",
    "    \"ObjectUse\",\n",
    "    \"PartOf\",\n",
    "    \"ReceivesAction\",\n",
    "    \"RelatedTo\",\n",
    "    \"SymbolOf\",\n",
    "    \"UsedFor\",\n",
    "    \"isAfter\",\n",
    "    \"isBefore\",\n",
    "    \"isFilledBy\",\n",
    "    \"oEffect\",\n",
    "    \"oReact\",\n",
    "    \"oWant\",\n",
    "    \"xAttr\",\n",
    "    \"xEffect\",\n",
    "    \"xIntent\",\n",
    "    \"xNeed\",\n",
    "    \"xReact\",\n",
    "    \"xReason\",\n",
    "    \"xWant\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_sentence = \"PersonX smells smoke in his house\"\n",
    "# NOTE: Need to add the \"for PersonX\" at the end to help the search converge faster\n",
    "end_sentence = \"Firefighers puts out the fire, saving PersonX's house\"\n",
    "model_path = \"./comet-atomic_2020_BART\"\n",
    "result = bidirectional_bfs(start_sentence=start_sentence, end_sentence=end_sentence, top_k=5, thresh=0.95, model_path=model_path)\n",
    "\n",
    "# sample usage\n",
    "# print(\"model loading ...\")\n",
    "# comet = Comet(\"./comet-atomic_2020_BART\")\n",
    "# comet.model.zero_grad()\n",
    "# print(\"model loaded\")\n",
    "\n",
    "# queries = []\n",
    "# head = \"PersonX eats an apple\"\n",
    "# rel = \"isBefore\"\n",
    "# query = \"{} {} [GEN]\".format(head, rel)\n",
    "# queries.append(query)\n",
    "# print(queries)\n",
    "# # sentences = [\"I am a boy\", \"Where are you going\"]\n",
    "# embeddings = comet.str2embeddings(sentences)\n",
    "# score = cosine_similarity(embeddings[0], embeddings[1])\n",
    "# print(\"score: \", score)\n",
    "# results, embeddings = comet.generate(queries, decode_method=\"beam\", num_generate=5)\n",
    "# print(\"embeddings.shape: \", embeddings.shape)\n",
    "# print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
