{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = a.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    return torch.dot(a,b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "def trim_batch(\n",
    "    input_ids, pad_token_id, attention_mask=None,\n",
    "):\n",
    "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
    "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "    if attention_mask is None:\n",
    "        return input_ids[:, keep_column_mask]\n",
    "    else:\n",
    "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
    "\n",
    "def use_task_specific_params(model, task):\n",
    "    \"\"\"Update config with summarization specific params.\"\"\"\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "\n",
    "    if task_specific_params is not None:\n",
    "        pars = task_specific_params.get(task, {})\n",
    "        #logger.info(f\"using task specific params for {task}: {pars}\")\n",
    "        model.config.update(pars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comet:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        task = \"summarization\"\n",
    "        use_task_specific_params(self.model, task)\n",
    "        self.batch_size = 1\n",
    "        self.decoder_start_token_id = None\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            queries,\n",
    "            decode_method=\"beam\",\n",
    "            num_generate=5,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Given list of queries, this generates num_generate sentences for each query.\n",
    "        Returns:\n",
    "            decs: list of lists of generated sentences (B lists)\n",
    "            decs_embeddings [B, L, H]: embeddings for the sentences\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            examples = queries\n",
    "\n",
    "            decs = []\n",
    "            decs_embeddings = []\n",
    "            for batch in list(chunks(examples, self.batch_size)):\n",
    "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "\n",
    "                summaries = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_start_token_id=self.decoder_start_token_id,\n",
    "                    num_beams=num_generate,\n",
    "                    num_return_sequences=num_generate,\n",
    "                    )\n",
    "\n",
    "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                decs.append(dec)\n",
    "\n",
    "                # converts list of strings to dictionary containing input_ids, masks, etc for input to transformer\n",
    "                dec_batch = self.tokenizer(dec, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "                dec_input_ids, _ = trim_batch(**dec_batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "                dec_embeddings = self.ids2embeddings(dec_input_ids)\n",
    "                decs_embeddings.append(dec_embeddings)\n",
    "        \n",
    "            # Return output as string as well as string converted to embeddings\n",
    "            # [B, L, H]\n",
    "            decs_embeddings = torch.cat(decs_embeddings, dim=1)\n",
    "            return decs, decs_embeddings\n",
    "    \n",
    "    def ids2embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (pytorch tensor [B, L])\n",
    "        Returns:\n",
    "            result (pytorch tensor [B, L, H]) where H is output dimension of embedding layer\n",
    "        \"\"\"\n",
    "        return self.model.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    def str2embeddings(self, sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: list of strings, each string is a sentence\n",
    "        Returns:\n",
    "            result (pytorch tensor [B, L, H]): B is number of sentences, L is max length of sentences, H is number of dimensions of embedding layer\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer(sentences, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
    "        input_ids, _ = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
    "        embeddings = self.ids2embeddings(input_ids)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_bfs(start_sentence, end_sentence, top_k, thresh=0.95, model_path=\"comet-atomic_2020_BART\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        top_k: how many\n",
    "    \"\"\"\n",
    "    print(\"model loading ...\")\n",
    "    comet = Comet(model_path)\n",
    "    comet.model.zero_grad()\n",
    "    print(\"model loaded\")\n",
    "\n",
    "\n",
    "    embeddings = comet.str2embeddings([start_sentence, end_sentence])\n",
    "    start_embedding = embeddings[0]\n",
    "    end_embedding = embeddings[1]\n",
    "\n",
    "    forward_q = [(start_sentence, start_embedding)]\n",
    "    backward_q = [(end_sentence, end_embedding)]\n",
    "\n",
    "    all_forward_results = []\n",
    "    all_backward_results = []\n",
    "\n",
    "    while len(forward_q) > 0 or len(backward_q) > 0:\n",
    "        if len(forward_q) > 0:\n",
    "            forward_sentence, forward_embedding = forward_q.pop(0)\n",
    "            all_forward_results.append(forward_sentence)\n",
    "            rel = \"isBefore\"\n",
    "            queries = [\"{} {} [GEN]\".format(forward_sentence, rel)]\n",
    "            forward_results, _ = comet.generate(queries, decode_method=\"beam\", num_generate=top_k)\n",
    "        \n",
    "        if len(backward_q) > 0:\n",
    "            backward_sentence, backward_embedding = backward_q.pop(0)\n",
    "            all_backward_results.append(backward_sentence)\n",
    "            rel = \"isAfter\"\n",
    "            queries = [\"{} {} [GEN]\".format(backward_sentence, rel)]\n",
    "            backward_results, _ = comet.generate(queries, decode_method=\"beam\", num_generate=top_k)\n",
    "        \n",
    "        similarity_score = cosine_similarity(forward_embedding, backward_embedding)\n",
    "        if similarity_score >= thresh:\n",
    "            print(\"{} and {} had similarity score of {}\".format(forward_sentence, backward_sentence, similarity_score))\n",
    "            final_forward_sentences = backtrack(all_forward_results, top_k)\n",
    "            final_backward_sentences = backtrack(all_backward_results, top_k)\n",
    "\n",
    "            return final_forward_sentences + final_backward_sentences[::-1][1:]\n",
    "        \n",
    "        # Need to ensure forward and backward sentences have same dimensions for computing\n",
    "        # cosine similarity\n",
    "        for forward_result, backward_result in zip(forward_results[0], backward_results[0]):\n",
    "            forward_embedding, backward_embedding = comet.str2embeddings([forward_result, backward_result])\n",
    "            forward_q.append((forward_result, forward_embedding))\n",
    "            backward_q.append((backward_result, backward_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(all_sentences, b):\n",
    "    \"\"\"\n",
    "    Do integer math to backtrack/recover path taken to reach the last sentence\n",
    "    Starting from the last node reached in the graph using BFS, where index represents\n",
    "    the ith node that was discovered in the graph during the bfs search (root node is index == 0)\n",
    "    the next index would be given by this equation: index := ceil(index / b) - 1\n",
    "    where b is branching factor or number of children each node has. We are taking advantage\n",
    "    of the fact that every node has b children. In our case, b = top_k where top_k is number of sentences we chose to generate for each query.\n",
    "    \"\"\"\n",
    "\n",
    "    # index starts at last node discovered which terminated our bfs search\n",
    "    ind = len(all_sentences) - 1\n",
    "    result = []\n",
    "    # now let's backtrack to recover the path taken to reach this terminating node\n",
    "    while ind != 0:\n",
    "        result.append(all_sentences[ind]) \n",
    "        ind = math.ceil(float(ind) / float(b)) - 1\n",
    "    # don't forget to add root-node\n",
    "    result.append(all_sentences[0])\n",
    "    return result[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_relations = [\n",
    "    \"AtLocation\",\n",
    "    \"CapableOf\",\n",
    "    \"Causes\",\n",
    "    \"CausesDesire\",\n",
    "    \"CreatedBy\",\n",
    "    \"DefinedAs\",\n",
    "    \"DesireOf\",\n",
    "    \"Desires\",\n",
    "    \"HasA\",\n",
    "    \"HasFirstSubevent\",\n",
    "    \"HasLastSubevent\",\n",
    "    \"HasPainCharacter\",\n",
    "    \"HasPainIntensity\",\n",
    "    \"HasPrerequisite\",\n",
    "    \"HasProperty\",\n",
    "    \"HasSubEvent\",\n",
    "    \"HasSubevent\",\n",
    "    \"HinderedBy\",\n",
    "    \"InheritsFrom\",\n",
    "    \"InstanceOf\",\n",
    "    \"IsA\",\n",
    "    \"LocatedNear\",\n",
    "    \"LocationOfAction\",\n",
    "    \"MadeOf\",\n",
    "    \"MadeUpOf\",\n",
    "    \"MotivatedByGoal\",\n",
    "    \"NotCapableOf\",\n",
    "    \"NotDesires\",\n",
    "    \"NotHasA\",\n",
    "    \"NotHasProperty\",\n",
    "    \"NotIsA\",\n",
    "    \"NotMadeOf\",\n",
    "    \"ObjectUse\",\n",
    "    \"PartOf\",\n",
    "    \"ReceivesAction\",\n",
    "    \"RelatedTo\",\n",
    "    \"SymbolOf\",\n",
    "    \"UsedFor\",\n",
    "    \"isAfter\",\n",
    "    \"isBefore\",\n",
    "    \"isFilledBy\",\n",
    "    \"oEffect\",\n",
    "    \"oReact\",\n",
    "    \"oWant\",\n",
    "    \"xAttr\",\n",
    "    \"xEffect\",\n",
    "    \"xIntent\",\n",
    "    \"xNeed\",\n",
    "    \"xReact\",\n",
    "    \"xReason\",\n",
    "    \"xWant\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading ...\n",
      "model loaded\n",
      " PersonX gets a new job and  PersonX gets a new job had similarity score of 0.9999999403953552\n",
      "['PersonX eats an apple', ' PersonX drinks some water', ' PersonX goes back to work', ' PersonX gets a promotion', ' PersonX gets a new job', ' PersonX has a lot of work', ' PersonX works all day', ' PersonX finishes work', 'PersonX goes home']\n"
     ]
    }
   ],
   "source": [
    "start_sentence = \"PersonX eats an apple\"\n",
    "end_sentence = \"PersonX goes home\"\n",
    "model_path = \"./comet-atomic_2020_BART\"\n",
    "result = bidirectional_bfs(start_sentence=start_sentence, end_sentence=end_sentence, top_k=5, thresh=0.95, model_path=model_path)\n",
    "print(result)\n",
    "# sample usage\n",
    "# print(\"model loading ...\")\n",
    "# comet = Comet(\"./comet-atomic_2020_BART\")\n",
    "# comet.model.zero_grad()\n",
    "# print(\"model loaded\")\n",
    "\n",
    "# queries = []\n",
    "# head = \"PersonX eats an apple\"\n",
    "# rel = \"isBefore\"\n",
    "# query = \"{} {} [GEN]\".format(head, rel)\n",
    "# queries.append(query)\n",
    "# print(queries)\n",
    "# # sentences = [\"I am a boy\", \"Where are you going\"]\n",
    "# embeddings = comet.str2embeddings(sentences)\n",
    "# score = cosine_similarity(embeddings[0], embeddings[1])\n",
    "# print(\"score: \", score)\n",
    "# results, embeddings = comet.generate(queries, decode_method=\"beam\", num_generate=5)\n",
    "# print(\"embeddings.shape: \", embeddings.shape)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
